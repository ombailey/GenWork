{"ast":null,"code":"/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nimport { add, greaterEqual, mul, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport class GaussianNoise extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.stddev = args.stddev;\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      stddev: this.stddev\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      const noised = () => add(K.randomNormal(input.shape, 0, this.stddev), input);\n      const output = K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      return output;\n    });\n  }\n}\n/** @nocollapse */\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport class GaussianDropout extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      rate: this.rate\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      if (this.rate > 0 && this.rate < 1) {\n        const noised = () => {\n          const stddev = Math.sqrt(this.rate / (1 - this.rate));\n          return mul(input, K.randomNormal(input.shape, 1, stddev));\n        };\n        return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      }\n      return input;\n    });\n  }\n}\n/** @nocollapse */\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\nexport class AlphaDropout extends Layer {\n  constructor(args) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n    this.noiseShape = args.noiseShape;\n  }\n  _getNoiseShape(inputs) {\n    return this.noiseShape || getExactlyOneTensor(inputs).shape;\n  }\n  computeOutputShape(inputShape) {\n    return inputShape;\n  }\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {\n      rate: this.rate\n    };\n    Object.assign(config, baseConfig);\n    return config;\n  }\n  call(inputs, kwargs) {\n    return tidy(() => {\n      if (this.rate < 1 && this.rate > 0) {\n        const noiseShape = this._getNoiseShape(inputs);\n        const droppedInputs = () => {\n          const input = getExactlyOneTensor(inputs);\n          const alpha = 1.6732632423543772848170429916717;\n          const scale = 1.0507009873554804934193349852946;\n          const alphaP = -alpha * scale;\n          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n          keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n          // Get affine transformation params.\n          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n          const b = -a * alphaP * this.rate;\n          // Apply mask.\n          const x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\n          return add(mul(x, a), b);\n        };\n        return K.inTrainPhase(droppedInputs, () => getExactlyOneTensor(inputs), kwargs['training'] || false);\n      }\n      return inputs;\n    });\n  }\n}\n/** @nocollapse */\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);","map":{"version":3,"mappings":"AAAA;;;;;;;;;AAUA;;;AAIA,SAAQA,GAAG,EAAEC,YAAY,EAAEC,GAAG,EAAEC,aAAa,EAAEC,aAAa,EAAUC,IAAI,QAAO,uBAAuB;AAExG,OAAO,KAAKC,CAAC,MAAM,yBAAyB;AAC5C,SAAQC,KAAK,QAAkB,oBAAoB;AAGnD,SAAQC,mBAAmB,QAAO,sBAAsB;AAOxD,OAAM,MAAOC,aAAc,SAAQF,KAAK;EAKtCG,YAAYC,IAAuB;IACjC,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAACC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAACC,MAAM,GAAGF,IAAI,CAACE,MAAM;EAC3B;EAEAC,kBAAkB,CAACC,UAAyB;IAC1C,OAAOA,UAAU;EACnB;EAEAC,SAAS;IACP,MAAMC,UAAU,GAAG,KAAK,CAACD,SAAS,EAAE;IACpC,MAAME,MAAM,GAAG;MAACL,MAAM,EAAE,IAAI,CAACA;IAAM,CAAC;IACpCM,MAAM,CAACC,MAAM,CAACF,MAAM,EAAED,UAAU,CAAC;IACjC,OAAOC,MAAM;EACf;EAEAG,IAAI,CAACC,MAAuB,EAAEC,MAAc;IAC1C,OAAOlB,IAAI,CAAC,MAAK;MACf,IAAI,CAACmB,cAAc,CAACF,MAAM,EAAEC,MAAM,CAAC;MACnC,MAAME,KAAK,GAAGjB,mBAAmB,CAACc,MAAM,CAAC;MACzC,MAAMI,MAAM,GAAG,MACX1B,GAAG,CAACM,CAAC,CAACqB,YAAY,CAACF,KAAK,CAACG,KAAK,EAAE,CAAC,EAAE,IAAI,CAACf,MAAM,CAAC,EAAEY,KAAK,CAAC;MAC3D,MAAMI,MAAM,GACRvB,CAAC,CAACwB,YAAY,CAACJ,MAAM,EAAE,MAAMD,KAAK,EAAEF,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC;MACpE,OAAOM,MAAM;IACf,CAAC,CAAC;EACJ;;AA/BA;AACOpB,uBAAS,GAAG,eAAe;AAgCpCL,aAAa,CAAC2B,aAAa,CAACtB,aAAa,CAAC;AAO1C,OAAM,MAAOuB,eAAgB,SAAQzB,KAAK;EAKxCG,YAAYC,IAAyB;IACnC,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAACC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAACqB,IAAI,GAAGtB,IAAI,CAACsB,IAAI;EACvB;EAEAnB,kBAAkB,CAACC,UAAyB;IAC1C,OAAOA,UAAU;EACnB;EAEAC,SAAS;IACP,MAAMC,UAAU,GAAG,KAAK,CAACD,SAAS,EAAE;IACpC,MAAME,MAAM,GAAG;MAACe,IAAI,EAAE,IAAI,CAACA;IAAI,CAAC;IAChCd,MAAM,CAACC,MAAM,CAACF,MAAM,EAAED,UAAU,CAAC;IACjC,OAAOC,MAAM;EACf;EAEAG,IAAI,CAACC,MAAuB,EAAEC,MAAc;IAC1C,OAAOlB,IAAI,CAAC,MAAK;MACf,IAAI,CAACmB,cAAc,CAACF,MAAM,EAAEC,MAAM,CAAC;MACnC,MAAME,KAAK,GAAGjB,mBAAmB,CAACc,MAAM,CAAC;MACzC,IAAI,IAAI,CAACW,IAAI,GAAG,CAAC,IAAI,IAAI,CAACA,IAAI,GAAG,CAAC,EAAE;QAClC,MAAMP,MAAM,GAAG,MAAK;UAClB,MAAMb,MAAM,GAAGqB,IAAI,CAACC,IAAI,CAAC,IAAI,CAACF,IAAI,IAAI,CAAC,GAAG,IAAI,CAACA,IAAI,CAAC,CAAC;UACrD,OAAO/B,GAAG,CAACuB,KAAK,EAAEnB,CAAC,CAACqB,YAAY,CAACF,KAAK,CAACG,KAAK,EAAE,CAAC,EAAEf,MAAM,CAAC,CAAC;QAC3D,CAAC;QACD,OAAOP,CAAC,CAACwB,YAAY,CAACJ,MAAM,EAAE,MAAMD,KAAK,EAAEF,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC;;MAEzE,OAAOE,KAAK;IACd,CAAC,CAAC;EACJ;;AAlCA;AACOO,yBAAS,GAAG,iBAAiB;AAmCtC5B,aAAa,CAAC2B,aAAa,CAACC,eAAe,CAAC;AAY5C;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA6BA,OAAM,MAAOI,YAAa,SAAQ7B,KAAK;EAMrCG,YAAYC,IAAsB;IAChC,KAAK,CAACA,IAAI,CAAC;IACX,IAAI,CAACC,eAAe,GAAG,IAAI;IAC3B,IAAI,CAACqB,IAAI,GAAGtB,IAAI,CAACsB,IAAI;IACrB,IAAI,CAACI,UAAU,GAAG1B,IAAI,CAAC0B,UAAU;EACnC;EAEAC,cAAc,CAAChB,MAAuB;IACpC,OAAO,IAAI,CAACe,UAAU,IAAI7B,mBAAmB,CAACc,MAAM,CAAC,CAACM,KAAK;EAC7D;EAEAd,kBAAkB,CAACC,UAAyB;IAC1C,OAAOA,UAAU;EACnB;EAEAC,SAAS;IACP,MAAMC,UAAU,GAAG,KAAK,CAACD,SAAS,EAAE;IACpC,MAAME,MAAM,GAAG;MAACe,IAAI,EAAE,IAAI,CAACA;IAAI,CAAC;IAChCd,MAAM,CAACC,MAAM,CAACF,MAAM,EAAED,UAAU,CAAC;IACjC,OAAOC,MAAM;EACf;EAEAG,IAAI,CAACC,MAAuB,EAAEC,MAAc;IAC1C,OAAOlB,IAAI,CAAC,MAAK;MACf,IAAI,IAAI,CAAC4B,IAAI,GAAG,CAAC,IAAI,IAAI,CAACA,IAAI,GAAG,CAAC,EAAE;QAClC,MAAMI,UAAU,GAAG,IAAI,CAACC,cAAc,CAAChB,MAAM,CAAC;QAE9C,MAAMiB,aAAa,GAAG,MAAK;UACzB,MAAMd,KAAK,GAAGjB,mBAAmB,CAACc,MAAM,CAAC;UAEzC,MAAMkB,KAAK,GAAG,iCAAiC;UAC/C,MAAMC,KAAK,GAAG,iCAAiC;UAE/C,MAAMC,MAAM,GAAG,CAACF,KAAK,GAAGC,KAAK;UAE7B,IAAIE,OAAO,GAAG1C,YAAY,CAACE,aAAa,CAACkC,UAAU,CAAC,EAAE,IAAI,CAACJ,IAAI,CAAC;UAEhEU,OAAO,GAAGrC,CAAC,CAACsC,IAAI,CAACD,OAAO,EAAE,SAAS,CAAC,CAAC,CAAE;UAEvC;UACA,MAAME,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,IAAI,CAACZ,IAAI,KAAK,CAAC,GAAG,IAAI,CAACA,IAAI,GAAGS,MAAM,IAAI,CAAC,CAAC,KAAK,CAAC,GAAG;UACnE,MAAMI,CAAC,GAAG,CAACD,CAAC,GAAGH,MAAM,GAAG,IAAI,CAACT,IAAI;UAEjC;UACA,MAAMc,CAAC,GAAG/C,GAAG,CAACE,GAAG,CAACuB,KAAK,EAAEkB,OAAO,CAAC,EAAEzC,GAAG,CAACF,GAAG,CAAC2C,OAAO,EAAE,CAAC,CAAC,CAAC,EAAED,MAAM,CAAC,CAAC;UAEjE,OAAO1C,GAAG,CAACE,GAAG,CAAC6C,CAAC,EAAEF,CAAC,CAAC,EAAEC,CAAC,CAAC;QAC1B,CAAC;QACD,OAAOxC,CAAC,CAACwB,YAAY,CACjBS,aAAa,EAAE,MAAM/B,mBAAmB,CAACc,MAAM,CAAC,EAChDC,MAAM,CAAC,UAAU,CAAC,IAAI,KAAK,CAAC;;MAElC,OAAOD,MAAM;IACf,CAAC,CAAC;EACJ;;AA3DA;AACOc,sBAAS,GAAG,cAAc;AA4DnChC,aAAa,CAAC2B,aAAa,CAACK,YAAY,CAAC","names":["add","greaterEqual","mul","randomUniform","serialization","tidy","K","Layer","getExactlyOneTensor","GaussianNoise","constructor","args","supportsMasking","stddev","computeOutputShape","inputShape","getConfig","baseConfig","config","Object","assign","call","inputs","kwargs","invokeCallHook","input","noised","randomNormal","shape","output","inTrainPhase","registerClass","GaussianDropout","rate","Math","sqrt","AlphaDropout","noiseShape","_getNoiseShape","droppedInputs","alpha","scale","alphaP","keptIdx","cast","a","b","x"],"sources":["/Users/omarbailey/node_modules/tfjs-layers/src/layers/noise.ts"],"sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\n\nimport {add, greaterEqual, mul, randomUniform, serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\n\nimport * as K from '../backend/tfjs_backend';\nimport {Layer, LayerArgs} from '../engine/topology';\nimport {Shape} from '../keras_format/common';\nimport {Kwargs} from '../types';\nimport {getExactlyOneTensor} from '../utils/types_utils';\n\nexport declare interface GaussianNoiseArgs extends LayerArgs {\n  /** Standard Deviation.  */\n  stddev: number;\n}\n\nexport class GaussianNoise extends Layer {\n  /** @nocollapse */\n  static className = 'GaussianNoise';\n  readonly stddev: number;\n\n  constructor(args: GaussianNoiseArgs) {\n    super(args);\n    this.supportsMasking = true;\n    this.stddev = args.stddev;\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {stddev: this.stddev};\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      const noised = () =>\n          add(K.randomNormal(input.shape, 0, this.stddev), input);\n      const output =\n          K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      return output;\n    });\n  }\n}\nserialization.registerClass(GaussianNoise);\n\nexport declare interface GaussianDropoutArgs extends LayerArgs {\n  /** drop probability.  */\n  rate: number;\n}\n\nexport class GaussianDropout extends Layer {\n  /** @nocollapse */\n  static className = 'GaussianDropout';\n  readonly rate: number;\n\n  constructor(args: GaussianDropoutArgs) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {rate: this.rate};\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      this.invokeCallHook(inputs, kwargs);\n      const input = getExactlyOneTensor(inputs);\n      if (this.rate > 0 && this.rate < 1) {\n        const noised = () => {\n          const stddev = Math.sqrt(this.rate / (1 - this.rate));\n          return mul(input, K.randomNormal(input.shape, 1, stddev));\n        };\n        return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n      }\n      return input;\n    });\n  }\n}\nserialization.registerClass(GaussianDropout);\n\nexport declare interface AlphaDropoutArgs extends LayerArgs {\n  /** drop probability.  */\n  rate: number;\n  /**\n   * A 1-D `Tensor` of type `int32`, representing the\n   * shape for randomly generated keep/drop flags.\n   */\n  noiseShape?: Shape;\n}\n\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\nexport class AlphaDropout extends Layer {\n  /** @nocollapse */\n  static className = 'AlphaDropout';\n  readonly rate: number;\n  readonly noiseShape: Shape;\n\n  constructor(args: AlphaDropoutArgs) {\n    super(args);\n    this.supportsMasking = true;\n    this.rate = args.rate;\n    this.noiseShape = args.noiseShape;\n  }\n\n  _getNoiseShape(inputs: Tensor|Tensor[]) {\n    return this.noiseShape || getExactlyOneTensor(inputs).shape;\n  }\n\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\n    return inputShape;\n  }\n\n  getConfig() {\n    const baseConfig = super.getConfig();\n    const config = {rate: this.rate};\n    Object.assign(config, baseConfig);\n    return config;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      if (this.rate < 1 && this.rate > 0) {\n        const noiseShape = this._getNoiseShape(inputs);\n\n        const droppedInputs = () => {\n          const input = getExactlyOneTensor(inputs);\n\n          const alpha = 1.6732632423543772848170429916717;\n          const scale = 1.0507009873554804934193349852946;\n\n          const alphaP = -alpha * scale;\n\n          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n\n          keptIdx = K.cast(keptIdx, 'float32');  // get default dtype.\n\n          // Get affine transformation params.\n          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n          const b = -a * alphaP * this.rate;\n\n          // Apply mask.\n          const x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\n\n          return add(mul(x, a), b);\n        };\n        return K.inTrainPhase(\n            droppedInputs, () => getExactlyOneTensor(inputs),\n            kwargs['training'] || false);\n      }\n      return inputs;\n    });\n  }\n}\nserialization.registerClass(AlphaDropout);\n"]},"metadata":{},"sourceType":"module","externalDependencies":[]}